{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import dask_geopandas as dgpd\n",
    "import scipy.stats as stats\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "bp = Path(os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def mkdir_except(folder_name):  # function to create folders and ignore if folder exists\n",
    "\n",
    "    try:\n",
    "        os.mkdir(folder_name)\n",
    "        #print(\"Folder {} created. \".format(folder_name))\n",
    "\n",
    "    except:\n",
    "        print(\"Folder {} already exists. \\n\".format(folder_name))\n",
    "\n",
    "def download_save_zip(url, folder_path):  # function for downloading url to a path\n",
    "\n",
    "    file_name = url.split('/')[-1]\n",
    "    sp = folder_path / file_name\n",
    "\n",
    "    if os.path.isfile(sp):\n",
    "        print(\"{} already downloaded \\n\".format(sp))\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            req = urllib.request.urlopen(url)\n",
    "            print('Downloading: {} '.format(url))\n",
    "            data = req.read()\n",
    "            req.close()\n",
    "\n",
    "            local = open(sp, 'wb')\n",
    "            local.write(data)\n",
    "            local.close()\n",
    "\n",
    "        except urllib.error.HTTPError:\n",
    "            print(\"HTTPError for {} \".format(url))\n",
    "\n",
    "        except urllib.error.URLError:\n",
    "            print(\"URLError for {} \".format(url))\n",
    "\n",
    "# list of counties for scores to be run on. Below are counties within the 263m screening area\n",
    "county_names = [\n",
    "'Ventura'\n",
    "]\n",
    "\n",
    "date = input(\"Enter desired start date (YYYY-MM-DD): \")\n",
    "edf_name = 'EDF.zip'\n",
    "xy_name = 'GeoXY.zip'\n",
    "\n",
    "# set base geotracker urls and create the path for geotracker downloads (edf AND xy)\n",
    "\n",
    "geotracker_edf_url = \"https://geotracker.waterboards.ca.gov/data_download/edf_by_county/\"\n",
    "geo_edf_path = bp / 'geotracker_edf_results'\n",
    "mkdir_except(geo_edf_path)\n",
    "\n",
    "geotracker_xy_url = \"https://geotracker.waterboards.ca.gov/data_download/geo_by_county/\"\n",
    "geo_xy_path = bp / 'geotracker_xy'\n",
    "mkdir_except(geo_xy_path)\n",
    "\n",
    "def dl_geotracker(url_start, clist, url_alt, folder_path):\n",
    "\n",
    "    urlList = []\n",
    "\n",
    "    for i in clist:\n",
    "        url = url_start + i + url_alt\n",
    "        urlList.append(url)\n",
    "\n",
    "    for j in urlList:\n",
    "        download_save_zip(j, folder_path)\n",
    "\n",
    "# runs download for geotracker sample resutls (edf) and xy locations\n",
    "print('Downloading GeoTracker Data: \\n')\n",
    "dl_geotracker(geotracker_edf_url, county_names, edf_name, geo_edf_path)\n",
    "dl_geotracker(geotracker_xy_url, county_names, xy_name, geo_xy_path)\n",
    "\n",
    "# set base gama results url and creates a path for them to be downloaded to\n",
    "gama_base_url = 'https://gamagroundwater.waterboards.ca.gov/gama/data_download/'\n",
    "gama_res_path = bp / 'gama_results'\n",
    "mkdir_except(gama_res_path)\n",
    "\n",
    "# alternate url substrings for each dataset from GAMA\n",
    "gama_alt_urls = [\n",
    "    'ddw_',\n",
    "    'dpr_',\n",
    "    'dwr_',\n",
    "    'gama_dom_',\n",
    "    'gama_sp-study_',\n",
    "    'gama_usgs_',\n",
    "    'localgw_',\n",
    "    'usgs_nwis_',\n",
    "    'wb_cleanup_',\n",
    "    'wb_ilrp_',\n",
    "    'wrd_'\n",
    "]\n",
    "\n",
    "# gama results with dl_Save_zip()\n",
    "def dl_gama_results(start_url, clist, alt_urls, dl_path):\n",
    "\n",
    "    url_list = []\n",
    "\n",
    "    pref = 'gama_'\n",
    "    suf = '_v2.zip'\n",
    "\n",
    "    for c in clist:\n",
    "        c = c.lower()\n",
    "\n",
    "        for au in alt_urls:\n",
    "            url = start_url + pref + au + c + suf\n",
    "            url_list.append(url)\n",
    "\n",
    "    for url in url_list:\n",
    "        download_save_zip(url, dl_path)\n",
    "\n",
    "\n",
    "# Runs downloads for GAMA sample results\n",
    "print('Downloading GAMA sample results: \\n')\n",
    "dl_gama_results(gama_base_url, county_names, gama_alt_urls, gama_res_path)\n",
    "\n",
    "# set base GAMA xy url and path for downloads\n",
    "gama_xy_url = 'https://gamagroundwater.waterboards.ca.gov/gama/data_download/gama_location_construction_v2.zip'\n",
    "gama_xy_path = bp / 'gama_xy'\n",
    "mkdir_except(gama_xy_path)\n",
    "\n",
    "# runs download for GAMA xy data\n",
    "print('Downloading GAMA XY data: \\n')\n",
    "gama_xy = download_save_zip(gama_xy_url, gama_xy_path)\n",
    "\n",
    "\n",
    "# creation of sample ID column (SID)\n",
    "def sid_col(df):\n",
    "    df['SID'] = df['WID'].astype(\n",
    "        str) + ' ' + df['LOGDATE'].astype(str) + ' ' + df['PARLABEL'].astype(str)\n",
    "\n",
    "\n",
    "def create_gama_table(p): # loads gama results data to dataframe\n",
    "\n",
    "    print('Loading: {} '.format(p))\n",
    "\n",
    "    try:\n",
    "        column_list = ['GM_WELL_ID', 'GM_CHEMICAL_VVL',\n",
    "                        'GM_RESULT', 'GM_RESULT_UNITS', 'GM_SAMP_COLLECTION_DATE']\n",
    "\n",
    "        df = pd.read_csv(p, sep='\\t', lineterminator='\\n', encoding='unicode_escape',\n",
    "                            usecols=column_list, low_memory=False, parse_dates=['GM_SAMP_COLLECTION_DATE'])\n",
    "        df = df[df['GM_RESULT'] != 0]\n",
    "        df['GM_RESULT'] = pd.to_numeric(df['GM_RESULT'], errors='coerce')\n",
    "        df['GM_RESULT'] = df['GM_RESULT'].notna()\n",
    "        df['GM_RESULT'] = df['GM_RESULT'].astype(float)\n",
    "        df = df[df['GM_SAMP_COLLECTION_DATE'] >= date]\n",
    "\n",
    "        return df\n",
    "\n",
    "    except:\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(p, sep='\\t', lineterminator='\\n', usecols=column_list,\n",
    "                                low_memory=False, parse_dates=['GM_SAMP_COLLECTION_DATE'])\n",
    "            df = df[df['GM_RESULT'] != 0]\n",
    "            df['GM_RESULT'] = pd.to_numeric(df['GM_RESULT'], errors='coerce')\n",
    "            df['GM_RESULT'] = df['GM_RESULT'].notna()\n",
    "            df['GM_RESULT'] = df['GM_RESULT'].astype(float)\n",
    "            df = df[df['GM_SAMP_COLLECTION_DATE'] >= date]\n",
    "\n",
    "            return df\n",
    "\n",
    "        except:\n",
    "            print('{} corrupt. Data likely does not exist \\n'.format(p))\n",
    "\n",
    "\n",
    "# function to concat gama result datasets\n",
    "\n",
    "def concat_gama_data(files):\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for i in files:\n",
    "        j = create_gama_table(i)\n",
    "        if j is not None:\n",
    "            df_list.append(j)\n",
    "    print('')\n",
    "    print('Combining GAMA sample results: \\n')\n",
    "    concatDF = pd.concat(df_list, axis=0)\n",
    "\n",
    "    for df in df_list:\n",
    "        del df\n",
    "\n",
    "    return concatDF\n",
    "\n",
    "\n",
    "# collect gama results files and concat them\n",
    "print('Loading GAMA sample results: \\n')\n",
    "gama_files = gama_res_path.glob('**/*.zip')\n",
    "gama_results = concat_gama_data(gama_files)\n",
    "\n",
    "\n",
    "# rename columns to their Geotracker counterparts and add SID column\n",
    "print('Renaming GAMA columns and creating SID: \\n')\n",
    "column_dict = {'GM_WELL_ID': 'WID', 'GM_CHEMICAL_VVL': 'PARLABEL', 'GM_RESULT': 'PARVAL',\n",
    "                'GM_SAMP_COLLECTION_DATE': 'LOGDATE', 'GM_RESULT_UNITS': 'UNITS'}\n",
    "\n",
    "gama_results.rename(columns=column_dict, inplace=True)\n",
    "sid_col(gama_results)\n",
    "\n",
    "\n",
    "# function for loading geotradcker edf tables\n",
    "def create_edf_table(p):\n",
    "    print('Loading: {} '.format(p))\n",
    "    try:\n",
    "        column_list = ['GLOBAL_ID', 'FIELD_PT_NAME',\n",
    "                    'LOGDATE', 'PARLABEL', 'PARVAL', 'UNITS']\n",
    "\n",
    "        df = pd.read_csv(p, sep='\\t', lineterminator='\\n', encoding='unicode_escape', usecols=column_list, parse_dates=[\n",
    "            'LOGDATE'])\n",
    "        df = df[df['PARVAL'] != 0]\n",
    "        df['PARVAL'] = pd.to_numeric(df['PARVAL'], errors='coerce')\n",
    "        df['PARVAL'] = df['PARVAL'].notna()\n",
    "        df['PARVAL'] = df['PARVAL'].astype(float)\n",
    "        df = df[df['LOGDATE'] >= date]\n",
    "        df['WID'] = df['GLOBAL_ID'] + '-' + df['FIELD_PT_NAME']\n",
    "        df = df.drop(columns=['GLOBAL_ID', 'FIELD_PT_NAME'])\n",
    "\n",
    "        return df\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_csv(p, sep='\\t', lineterminator='\\n', usecols=column_list, parse_dates=[\n",
    "                'LOGDATE'])\n",
    "            df = df[df['PARVAL'] != 0]\n",
    "            df['PARVAL'] = pd.to_numeric(df['PARVAL'], errors='coerce')\n",
    "            df['PARVAL'] = df['PARVAL'].notna()\n",
    "            df['PARVAL'] = df['PARVAL'].astype(float)\n",
    "            df = df[df['LOGDATE'] >= date]\n",
    "            df['WID'] = df['GLOBAL_ID'] + '-' + df['FIELD_PT_NAME']\n",
    "            df = df.drop(columns=['GLOBAL_ID', 'FIELD_PT_NAME'])\n",
    "\n",
    "            return df\n",
    "\n",
    "        except:\n",
    "            print('{} corrupt. Data does not exist \\n'.format(p))\n",
    "\n",
    "\n",
    "# function to concat gama result datasets\n",
    "\n",
    "def concat_geo_data(files):\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for i in files:\n",
    "        j = create_edf_table(i)\n",
    "\n",
    "        if j is not None:\n",
    "            df_list.append(j)\n",
    "    print('Combining Geotracker EDF sample results: \\n')\n",
    "    concatDF = pd.concat(df_list, axis=0)\n",
    "\n",
    "    for df in df_list:\n",
    "        del df\n",
    "\n",
    "    return concatDF\n",
    "\n",
    "# Load and combine Geotracker EDF results\n",
    "print('Loading Geotracker EDF results \\n')\n",
    "edf_files = geo_edf_path.glob('**/*.zip')\n",
    "edf_results = concat_geo_data(edf_files)\n",
    "sid_col(edf_results)\n",
    "\n",
    "# Combines GAMA and Geotracker EDF results into one dataset\n",
    "print('Combining GAMA and Geotracker EDF results \\n')\n",
    "samples = pd.concat([edf_results, gama_results], ignore_index=True)\n",
    "\n",
    "for i in [edf_results, gama_results]:\n",
    "    del i\n",
    "\n",
    "samples['SID'] = samples['SID'].astype(str)\n",
    "\n",
    "# selects desired samples and drops duplicates in SID, grabbing highest results value of dupes\n",
    "print('Selecting desired samples and dropping duplicates \\n')\n",
    "samples = samples.sort_values(by=['SID'])\n",
    "samples = samples.sort_values(by='PARVAL', ascending=False)\n",
    "samples = samples.drop_duplicates(subset='SID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gama sample loading\n",
    "def sid_col(df):\n",
    "    df['SID'] = df['WID'].astype(\n",
    "        str) + ' ' + df['LOGDATE'].astype(str) + ' ' + df['PARLABEL'].astype(str)\n",
    "\n",
    "\n",
    "def create_gama_table(p): # loads gama results data to dataframe\n",
    "\n",
    "    print('Loading: {} '.format(p))\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(p, sep='\\t', lineterminator='\\n', encoding='unicode_escape', low_memory=False, parse_dates=['GM_SAMP_COLLECTION_DATE'])\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_csv(p, sep='\\t', lineterminator='\\n', low_memory=False, parse_dates=['GM_SAMP_COLLECTION_DATE'])\n",
    "            \n",
    "        except:\n",
    "            print('Corrupt: {} '.format(p))\n",
    "            df = None\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def concat_gama_data(files): # function to concat gama result datasets\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for i in files:\n",
    "        j = create_gama_table(i)\n",
    "        if j is not None:\n",
    "            df_list.append(j)\n",
    "\n",
    "    print('\\nCombining GAMA sample results: \\n')\n",
    "\n",
    "    concatDF = pd.concat(df_list, axis=0)\n",
    "\n",
    "    return concatDF\n",
    "\n",
    "\n",
    "# collect gama results files and concat them\n",
    "print('Loading GAMA sample results: \\n')\n",
    "gama_files = gama_res_path.glob('**/*.zip')\n",
    "gama_results = concat_gama_data(gama_files)\n",
    "\n",
    "\n",
    "def clean_filter_gama_resutls(df):\n",
    "\n",
    "    column_dict = {'GM_WELL_ID': 'WID', 'GM_CHEMICAL_VVL': 'PARLABEL','GM_RESULT_MODIFIER': 'PARVQ','GM_RESULT': 'PARVAL','GM_REPORTING_LIMIT':'REPDL',\n",
    "            'GM_SAMP_COLLECTION_DATE': 'LOGDATE', 'GM_RESULT_UNITS': 'UNITS'}\n",
    "\n",
    "    df = df.rename(columns=column_dict)\n",
    "\n",
    "    columns = ['WID', 'PARLABEL', 'PARVQ', 'PARVAL', 'REPDL', 'LOGDATE', 'UNITS', 'SID']\n",
    "\n",
    "    df = df[\n",
    "        (df['LOGDATE'] >= date) &\n",
    "        (df['PARVQ'] != '<') &\n",
    "        (df['PARVAL'] != 0) &\n",
    "        (df['PARVAL'] > df['REPDL'])\n",
    "    ]\n",
    "\n",
    "    sid_col(df)\n",
    "    df = df[columns]\n",
    "    return df\n",
    "\n",
    "# rename columns to their Geotracker counterparts and add SID column\n",
    "print('Renaming GAMA columns and creating SIDs: \\n')\n",
    "gama_results = clean_filter_gama_resutls(gama_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edf sample loading\n",
    "\n",
    "\n",
    "def create_edf_table(p):\n",
    "\n",
    "    print('Loading: {} '.format(p))\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(p, sep='\\t', lineterminator='\\n', encoding='unicode_escape', parse_dates=[\n",
    "            'LOGDATE'])\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_csv(p, sep='\\t', lineterminator='\\n', parse_dates=[\n",
    "                'LOGDATE'])\n",
    "\n",
    "        except:\n",
    "            print('Corrupt: {} '.format(p))     \n",
    "\n",
    "            df = None\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def concat_geo_data(files): # function to concat gama result datasets\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for i in files:\n",
    "        j = create_edf_table(i)\n",
    "\n",
    "        if j is not None:\n",
    "            df_list.append(j)\n",
    "\n",
    "    print('Combining Geotracker EDF sample results: \\n')\n",
    "\n",
    "    concat_df = pd.concat(df_list, axis=0)\n",
    "\n",
    "    for df in df_list:\n",
    "        del df\n",
    "\n",
    "    return concat_df\n",
    "\n",
    "\n",
    "# Load and combine Geotracker EDF results\n",
    "print('Loading Geotracker EDF results \\n')\n",
    "\n",
    "edf_files = geo_edf_path.glob('**/*.zip')\n",
    "edf_results = concat_geo_data(edf_files)\n",
    "\n",
    "def clean_filter_edf_resutls(df):\n",
    "\n",
    "    columns = ['WID', 'PARLABEL', 'PARVQ', 'PARVAL', 'REPDL', 'LOGDATE', 'UNITS', 'SID']\n",
    "\n",
    "    df = df[\n",
    "        (df['LOGDATE'] >= date) &\n",
    "        (df['PARVQ'] != '<') &\n",
    "        (df['PARVAL'] != 0) &\n",
    "        (df['PARVAL'] > df['REPDL'])\n",
    "    ]\n",
    "    df['WID'] = df['GLOBAL_ID'] + '-' + df['FIELD_PT_NAME']\n",
    "    df = df.drop(columns=['GLOBAL_ID', 'FIELD_PT_NAME'])\n",
    "    sid_col(df)\n",
    "    df = df[columns]\n",
    "    return df\n",
    "\n",
    "# rename columns to their Geotracker counterparts and add SID column\n",
    "print('Renaming EDF columns and creating SIDs: \\n')\n",
    "edf_results = clean_filter_edf_resutls(edf_results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f86f1d05d54eb1d73251de2c58ff719ad58d8c49eae568a5e6dd785c8b72bc6f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('geoprj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
